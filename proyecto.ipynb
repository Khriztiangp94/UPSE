{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9096d9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab') \n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14daf87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/justmarkham/DAT8/master/data/sms.tsv\"\n",
    "df = pd.read_csv(url, sep='\\t', names=[\"label\", \"text\"])\n",
    "df['label'] = df['label'].map({'ham': 0, 'spam': 1})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e375a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Columnas:\", df.columns)\n",
    "print(\"\\nValores nulos por columna:\\n\", df.isna().sum())\n",
    "print(\"\\nClases:\\n\", df['label'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f307a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['length'] = df['text'].str.len()\n",
    "df[['text','length']].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d983845",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    # 1. Minúsculas\n",
    "    text = text.lower()\n",
    "    # 2. Eliminar todo lo que no sea letras o espacios\n",
    "    text = re.sub(r\"[^a-z\\s]\", \" \", text)\n",
    "    # 3. Tokenización\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    # 4. Eliminar stopwords y palabras cortas\n",
    "    tokens = [t for t in tokens if t not in stop_words and len(t) > 1]\n",
    "    # 5. Reconstrucción\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "# Aplicar al dataset\n",
    "df['clean_text'] = df['text'].astype(str).apply(clean_text)\n",
    "\n",
    "# Comparar original vs limpio\n",
    "df[['text','clean_text']].head(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b1278b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.to_csv(\"sms_clean.csv\", index=False)\n",
    "#print(\"Dataset limpio guardado como sms_clean.csv\")\n",
    "# Guardar el dataset limpio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d7fcd1",
   "metadata": {},
   "source": [
    "Selección de características con TF-IDF + modelo superficial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7caa71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "\n",
    "# 1. Vectorización TF-IDF\n",
    "vectorizer = TfidfVectorizer(max_features=5000)  # máximo 5000 palabras\n",
    "X = vectorizer.fit_transform(df['clean_text'])\n",
    "y = df['label']\n",
    "\n",
    "print(\"Matriz TF-IDF:\", X.shape)\n",
    "\n",
    "# 2. Selección de características con chi-cuadrado\n",
    "k = 1000  # número de features a conservar\n",
    "selector = SelectKBest(chi2, k=k)\n",
    "X_new = selector.fit_transform(X, y)\n",
    "\n",
    "print(\"Matriz después de selección:\", X_new.shape)\n",
    "\n",
    "# Obtener las palabras más importantes del dataset\n",
    "selected_features = [vectorizer.get_feature_names_out()[i] for i in selector.get_support(indices=True)]\n",
    "print(\"Ejemplo de features seleccionadas:\", selected_features[:100])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8855016d",
   "metadata": {},
   "source": [
    "Aplicacion de SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d3e452",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1. Separar en entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_new, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# 2. Definir y entrenar modelo SVM\n",
    "svm_model = SVC(kernel=\"linear\", C=1.0, random_state=42)\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "# 3. Predecir en test\n",
    "y_pred = svm_model.predict(X_test)\n",
    "\n",
    "# 4. Métricas\n",
    "print(\"🔹 Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\n🔹 Reporte de Clasificación:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "# 5. Matriz de confusión\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"Ham\", \"Spam\"], yticklabels=[\"Ham\", \"Spam\"])\n",
    "plt.xlabel(\"Predicción\")\n",
    "plt.ylabel(\"Real\")\n",
    "plt.title(\"Matriz de Confusión - SVM\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c8a515",
   "metadata": {},
   "source": [
    "APRENDIZAJE PROFUNDO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a61878",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn.functional as F\n",
    "from mlp import PyTorchMLP\n",
    "\n",
    "# 1. Convertir X y y a tensores\n",
    "X_train_tensor = torch.tensor(X_train.toarray(), dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.long)\n",
    "X_test_tensor = torch.tensor(X_test.toarray(), dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.long)\n",
    "\n",
    "# 2. Crear DataLoader\n",
    "train_data = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_data = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=64)\n",
    "\n",
    "# 3. Definir modelo\n",
    "num_features = X_train_tensor.shape[1]\n",
    "num_classes = 2\n",
    "model = PyTorchMLP(num_features, num_classes)\n",
    "\n",
    "# 4. Definir optimizador y función de pérdida\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 5. Entrenamiento\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss/len(train_loader):.4f}\")\n",
    "\n",
    "# 6. Evaluación\n",
    "model.eval()\n",
    "y_true, y_pred = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in test_loader:\n",
    "        outputs = model(X_batch)\n",
    "        preds = torch.argmax(F.softmax(outputs, dim=1), dim=1)\n",
    "        y_true.extend(y_batch.tolist())\n",
    "        y_pred.extend(preds.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a545a7",
   "metadata": {},
   "source": [
    "Presentacion de resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c027281",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"🔹 Accuracy (MLP):\", accuracy_score(y_true, y_pred))\n",
    "print(\"\\n🔹 Reporte de Clasificación (MLP):\\n\", classification_report(y_true, y_pred))\n",
    "\n",
    "# Matriz de confusión\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"Ham\", \"Spam\"], yticklabels=[\"Ham\", \"Spam\"])\n",
    "plt.xlabel(\"Predicción\")\n",
    "plt.ylabel(\"Real\")\n",
    "plt.title(\"Matriz de Confusión - MLP\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce19591c",
   "metadata": {},
   "source": [
    "APRENDIZAJE PROFUNDO CON LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251b5c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from collections import Counter\n",
    "from lstm import PyTorchLSTM\n",
    "\n",
    "\n",
    "# 1. Tokenización y vocabulario (sin torchtext)\n",
    "specials = [\"<unk>\", \"<pad>\"]\n",
    "counter = Counter()\n",
    "\n",
    "for text in df['clean_text']:\n",
    "    counter.update(text.split())\n",
    "\n",
    "itos = specials + [word for word, _ in counter.most_common()]\n",
    "stoi = {word: idx for idx, word in enumerate(itos)}\n",
    "\n",
    "def text_pipeline(x):\n",
    "    return [stoi.get(token, stoi[\"<unk>\"]) for token in x.split()]\n",
    "\n",
    "# Convertir dataset\n",
    "X_indices = [torch.tensor(text_pipeline(text), dtype=torch.long) for text in df['clean_text']]\n",
    "y_tensor = torch.tensor(df['label'].values, dtype=torch.long)\n",
    "\n",
    "# Padding\n",
    "X_padded = pad_sequence(X_indices, batch_first=True, padding_value=stoi[\"<pad>\"])\n",
    "\n",
    "# 2. Train / Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_padded, y_tensor, test_size=0.2, stratify=y_tensor, random_state=42\n",
    ")\n",
    "\n",
    "train_data = TensorDataset(X_train, y_train)\n",
    "test_data = TensorDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=64)\n",
    "\n",
    "# 3. Definir modelo LSTM\n",
    "vocab_size = len(stoi)\n",
    "embed_dim = 128\n",
    "hidden_dim = 64\n",
    "num_classes = 2\n",
    "model = PyTorchLSTM(vocab_size, embed_dim, hidden_dim, num_classes,\n",
    "                    num_layers=1, bidirectional=True).to(device)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 4. Entrenamiento\n",
    "epochs = 5\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss/len(train_loader):.4f}\")\n",
    "\n",
    "# 5. Evaluación\n",
    "model.eval()\n",
    "y_true, y_pred = [], []\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in test_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        outputs = model(X_batch)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        y_true.extend(y_batch.tolist())\n",
    "        y_pred.extend(preds.tolist())\n",
    "\n",
    "print(\"🔹 Accuracy (LSTM):\", accuracy_score(y_true, y_pred))\n",
    "print(\"\\n🔹 Reporte de Clasificación (LSTM):\\n\", classification_report(y_true, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a7e545",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Métricas\n",
    "print(\"🔹 Accuracy (LSTM):\", accuracy_score(y_true, y_pred))\n",
    "print(\"\\n🔹 Reporte de Clasificación (LSTM):\\n\", classification_report(y_true, y_pred))\n",
    "\n",
    "# Matriz de confusión\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "            xticklabels=[\"Ham\", \"Spam\"], yticklabels=[\"Ham\", \"Spam\"])\n",
    "plt.xlabel(\"Predicción\")\n",
    "plt.ylabel(\"Real\")\n",
    "plt.title(\"Matriz de Confusión - LSTM\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (mltelecom)",
   "language": "python",
   "name": "mltelecom"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
